{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbaseconda6de4b8f9c0484f53b411415811d6d548",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spoiler text classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "#import xgboost as xgb\n",
    "\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"75k43p5percent.csv\")\n",
    "df = pd.read_csv(\"csvCut/goodreads49k39percentSHUFFLE.csv\")#, nrows=15000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  has_spoiler                                           sentence\n",
       "0           0            1                                    son of a bitch!\n",
       "1           1            0  Every single time we refused to cave in to our...\n",
       "2           2            0  And Everly had both of these qualities in spad...\n",
       "3           3            0  For what it is, it is a pleasant enough read -...\n",
       "4           4            1                          They say the same saying."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>has_spoiler</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>son of a bitch!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Every single time we refused to cave in to our...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>And Everly had both of these qualities in spad...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>For what it is, it is a pleasant enough read -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>They say the same saying.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "docs = [[stemmer.stem(lemmatizer.lemmatize(t)) for t in word_tokenize(doc)] for doc in df[\"sentence\"].to_numpy()]\n",
    "\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = \" \".join(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=25000)\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# model = Pipeline([\n",
    "#     ('vectorizer', vectorizer),\n",
    "#     ('classifier', classifier)\n",
    "# ])\n",
    "\n",
    "model = classifier\n",
    "\n",
    "#X = df[\"sentence\"].to_numpy()\n",
    "X = vectorizer.fit_transform(docs).toarray()\n",
    "y = df[\"has_spoiler\"].to_numpy()\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test accuracy = 70.83%\n"
     ]
    }
   ],
   "source": [
    "model.fit(trainX, trainY)\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "acc = accuracy_score(testY, predictions)\n",
    "\n",
    "print('Test accuracy = {:.2%}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED TEST ON MADE UP SENTENCES (PRIOR TO LEMMATIZATION AND STEMMING)\n",
    "# SUCCESS\n",
    "\n",
    "# cntrl = np.array([\"The protagonist dies in the end.\", \"I liked this movie.\", \"Turns out, he was the villain all along\", \"Lara is a cool character\", \"Spoilers! The dog is actually a cat.\"])\n",
    "\n",
    "# docs = [[stemmer.stem(lemmatizer.lemmatize(t)) for t in word_tokenize(doc)] for doc in cntrl]\n",
    "\n",
    "# for i in range(len(docs)):\n",
    "#     docs[i] = \" \".join(docs[i])\n",
    "\n",
    "# cntrlX = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "# cntrlPred = model.predict(cntrlX)\n",
    "# cntrlPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}